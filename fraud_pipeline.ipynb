{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plot_params\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import load\n",
    "\n",
    "plot_params.apply_rcparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load('X_train.joblib')\n",
    "y_train = load('y_train.joblib')\n",
    "X_test = load('X_test.joblib')\n",
    "y_test = load('y_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipeline = Pipeline([\n",
    "   ('scaler', StandardScaler()),  # Scaling within the pipeline\n",
    "    ('logreg', LogisticRegression(solver='saga', max_iter=500))\n",
    "])\n",
    "\n",
    "logreg_params = {\n",
    "    'logreg__C': [0.1, 1, 10, 100],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "svc_params = {\n",
    "    'svc__C': [0.01, 0.1, 1, 10, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=12))\n",
    "])\n",
    "\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [50, 200], \n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Logistic Regression Parameters: {'logreg__C': 100, 'logreg__penalty': 'l2'}\n",
      "Best Logistic Regression Accuracy: 0.9710417014063044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frang\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_search_logreg = GridSearchCV(logreg_pipeline, logreg_params, cv=5, \n",
    "                                  scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "grid_search_logreg.fit(X_train, y_train)\n",
    "print(\"Best Logistic Regression Parameters:\", grid_search_logreg.best_params_)\n",
    "print(\"Best Logistic Regression Accuracy:\", grid_search_logreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frang\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=20. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Randomized Logistic Regression Parameters: {'logreg__penalty': 'l2', 'logreg__C': 100}\n",
      "Best Randomized Logistic Regression Accuracy: 0.9710417014063044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frang\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "random_search_logreg = RandomizedSearchCV(logreg_pipeline, logreg_params, \n",
    "                                          n_iter=20, cv=5, scoring='f1_macro', \n",
    "                                          n_jobs=-1, verbose=1, \n",
    "                                          random_state=42)\n",
    "random_search_logreg.fit(X_train, y_train)\n",
    "print(\"Best Randomized Logistic Regression Parameters:\", \n",
    "      random_search_logreg.best_params_)\n",
    "print(\"Best Randomized Logistic Regression Accuracy:\", \n",
    "      random_search_logreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best SVC Parameters: {'svc__C': 10}\n",
      "Best SVC Accuracy: 0.9701861747842166\n"
     ]
    }
   ],
   "source": [
    "grid_search_svc = GridSearchCV(svc_pipeline, svc_params, cv=5, \n",
    "                               scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "grid_search_svc.fit(X_train, y_train)\n",
    "print(\"Best SVC Parameters:\", grid_search_svc.best_params_)\n",
    "print(\"Best SVC Accuracy:\", grid_search_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frang\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 5 is smaller than n_iter=50. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Randomized SVC Parameters: {'svc__C': 10}\n",
      "Best Randomized SVC Accuracy: 0.9701861747842166\n"
     ]
    }
   ],
   "source": [
    "random_search_svc = RandomizedSearchCV(svc_pipeline, svc_params, n_iter=50, \n",
    "                                       cv=5, scoring='f1_macro', n_jobs=-1, \n",
    "                                       verbose=1, random_state=42)\n",
    "random_search_svc.fit(X_train, y_train)\n",
    "print(\"Best Randomized SVC Parameters:\", random_search_svc.best_params_)\n",
    "print(\"Best Randomized SVC Accuracy:\", random_search_svc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search_rf = GridSearchCV(rf_pipeline, rf_params, cv=5, \n",
    "                              scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(\"Best Random Forest Parameters:\", grid_search_rf.best_params_)\n",
    "print(\"Best Random Forest Accuracy:\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_rf = RandomizedSearchCV(rf_pipeline, rf_params, n_iter=20, \n",
    "                                      cv=5, scoring='f1_macro', n_jobs=-1, \n",
    "                                      verbose=1, random_state=12)\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "print(\"Best Randomized Random Forest Parameters:\", \n",
    "      random_search_rf.best_params_)\n",
    "print(\"Best Randomized Random Forest Accuracy:\", random_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Test Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     56864\n",
      "           1       0.09      0.92      0.17        98\n",
      "\n",
      "    accuracy                           0.98     56962\n",
      "   macro avg       0.55      0.95      0.58     56962\n",
      "weighted avg       1.00      0.98      0.99     56962\n",
      "\n",
      "Test Accuracy: 0.9846739931884414\n"
     ]
    }
   ],
   "source": [
    "best_logreg = grid_search_logreg.best_estimator_ \n",
    "y_pred_logreg = best_logreg.predict(X_test)\n",
    "print(\"\\nLogistic Regression Test Performance:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_svc \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_svc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m  \u001b[38;5;66;03m# Or random_search_svc.best_estimator_\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y_pred_svc \u001b[38;5;241m=\u001b[39m best_svc\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;66;03m# No need to scale X_test here either\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSVC Test Performance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "best_svc = grid_search_svc.best_estimator_\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "print(\"\\nSVC Test Performance:\")\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_rf\u001b[49m\u001b[38;5;241m.\u001b[39mbest_estimator_ \u001b[38;5;66;03m# Or random_search_rf.best_estimator_\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m best_rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRandom Forest Test Performance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_search_rf' is not defined"
     ]
    }
   ],
   "source": [
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "print(\"\\nRandom Forest Test Performance:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model features which are interesting? precision recall curve just predicting postive values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_logreg = best_logreg.decision_function(X_test) \n",
    "precision_logreg, recall_logreg, _ = \\\n",
    "    precision_recall_curve(y_test, y_score_logreg)\n",
    "auc_logreg = auc(recall_logreg, precision_logreg)\n",
    "avg_prec_logreg = average_precision_score(y_test, y_score_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_svc = best_svc.decision_function(X_test)  \n",
    "precision_svc, recall_svc, _ = \\\n",
    "    precision_recall_curve(y_test, y_score_svc)\n",
    "auc_svc = auc(recall_svc, precision_svc)\n",
    "avg_prec_svc = average_precision_score(y_test, y_score_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_score_rf)\n",
    "auc_rf = auc(recall_rf, precision_rf)\n",
    "avg_prec_rf = average_precision_score(y_test, y_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(recall_logreg, precision_logreg, \n",
    "         label=f'LogReg (AUC = {auc_logreg:.2f}, AP = {avg_prec_logreg:.2f})')\n",
    "plt.plot(recall_svc, precision_svc, \n",
    "         label=f'SVC (AUC = {auc_svc:.2f}, AP = {avg_prec_svc:.2f})')\n",
    "plt.plot(recall_rf, precision_rf, \n",
    "         label=f'Random Forest (AUC = {auc_rf:.2f}, AP = {avg_prec_rf:.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
